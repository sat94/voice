#!/usr/bin/env python3
"""Test du nouveau modÃ¨le Qwen2.5-72B vs ancien Llama-3.3-70B"""

import requests
import json
import time

# Configuration
API_URL = "http://localhost:8001"
API_KEY = "152445aasdaiaze145ae656ae2312aaaezaz32a132Ã¹aezÃ¹Ã¹aaeaeÃ¹Ã¹aeaea"

headers = {
    "X-API-Key": API_KEY,
    "Content-Type": "application/json"
}

def test_qwen_performance():
    """Test des performances de Qwen2.5-72B"""
    print("ğŸš€ Test du nouveau modÃ¨le Qwen2.5-72B\n")
    
    test_scenarios = [
        {
            "prompt": "Comment draguer intelligemment une fille dans un cafÃ© sans Ãªtre lourd ?",
            "category": "seduction",
            "expected_model": "Qwen"
        },
        {
            "prompt": "Analyse psychologique : pourquoi certaines personnes ont peur de l'engagement ?",
            "category": "psychology", 
            "expected_model": "Gemini"
        },
        {
            "prompt": "Donne-moi 5 techniques concrÃ¨tes pour Ãªtre plus confiant en rendez-vous",
            "category": "seduction",
            "expected_model": "Qwen"
        },
        {
            "prompt": "Comment gÃ©rer l'anxiÃ©tÃ© sociale dans les relations amoureuses ?",
            "category": "psychology",
            "expected_model": "Variable"
        }
    ]
    
    total_time = 0
    qwen_responses = 0
    gemini_responses = 0
    
    for i, scenario in enumerate(test_scenarios, 1):
        print(f"Test {i}/{len(test_scenarios)}: {scenario['category']}")
        print(f"Question: {scenario['prompt']}")
        
        start_time = time.time()
        
        try:
            response = requests.post(
                f"{API_URL}/ia/simple", 
                headers=headers, 
                json={"prompt": scenario['prompt']}
            )
            
            response_time = time.time() - start_time
            total_time += response_time
            
            if response.status_code == 200:
                result = response.json()
                
                category = result.get('category')
                ai_provider = result.get('ai_provider')
                response_text = result.get('response')
                
                print(f"âœ… CatÃ©gorie dÃ©tectÃ©e: {category}")
                print(f"âœ… IA utilisÃ©e: {ai_provider}")
                print(f"âœ… Temps de rÃ©ponse: {response_time:.2f}s")
                print(f"âœ… RÃ©ponse: {response_text[:150]}...")
                
                # Compter les modÃ¨les utilisÃ©s
                if "Qwen" in ai_provider:
                    qwen_responses += 1
                    print("ğŸ¯ Qwen2.5-72B utilisÃ© !")
                elif "google_gemini" in ai_provider:
                    gemini_responses += 1
                    print("ğŸ§  Google Gemini utilisÃ© !")
                
                # Ã‰valuer la qualitÃ© de la rÃ©ponse
                if len(response_text) > 100 and "dÃ©solÃ©e" not in response_text.lower():
                    print("âœ… RÃ©ponse de qualitÃ© dÃ©tectÃ©e")
                else:
                    print("âš ï¸ RÃ©ponse courte ou fallback")
                    
            else:
                print(f"âŒ Erreur: {response.status_code}")
                
        except Exception as e:
            print(f"âŒ Erreur: {e}")
        
        print("-" * 70)
    
    # Statistiques finales
    avg_time = total_time / len(test_scenarios)
    
    print(f"\nğŸ“Š Statistiques de performance:")
    print(f"âœ… Temps moyen de rÃ©ponse: {avg_time:.2f}s")
    print(f"âœ… RÃ©ponses Qwen2.5-72B: {qwen_responses}/{len(test_scenarios)}")
    print(f"âœ… RÃ©ponses Google Gemini: {gemini_responses}/{len(test_scenarios)}")
    print(f"âœ… RÃ©partition: {(qwen_responses/len(test_scenarios)*100):.1f}% Qwen, {(gemini_responses/len(test_scenarios)*100):.1f}% Gemini")

def test_qwen_vs_gemini_quality():
    """Comparaison qualitÃ© Qwen vs Gemini"""
    print("\nğŸ¥Š Comparaison qualitÃ© Qwen2.5 vs Gemini")
    
    # Question qui peut aller vers les deux
    prompt = "Comment dÃ©velopper son charisme naturel pour Ãªtre plus attirant ?"
    
    print(f"Question test: {prompt}\n")
    
    # Test avec routage normal
    try:
        response = requests.post(
            f"{API_URL}/ia/simple", 
            headers=headers, 
            json={"prompt": prompt}
        )
        
        if response.status_code == 200:
            result = response.json()
            
            print(f"ğŸ¤– Routage automatique:")
            print(f"   IA choisie: {result.get('ai_provider')}")
            print(f"   CatÃ©gorie: {result.get('category')}")
            print(f"   RÃ©ponse: {result.get('response')[:200]}...")
        else:
            print(f"âŒ Erreur: {response.status_code}")
            
    except Exception as e:
        print(f"âŒ Erreur: {e}")

def test_model_status():
    """VÃ©rifier le statut des modÃ¨les"""
    print("\nğŸ” Statut des modÃ¨les IA")
    
    try:
        response = requests.get(f"{API_URL}/ia/status")
        
        if response.status_code == 200:
            result = response.json()
            
            print(f"âœ… Google Gemini: {'Actif' if result['google_gemini']['configured'] else 'Inactif'}")
            print(f"âœ… DeepInfra: {'Actif' if result['deepinfra']['configured'] else 'Inactif'}")
            print(f"âœ… ModÃ¨le DeepInfra: {result['deepinfra']['model']}")
            print(f"âœ… Mode fallback: {'Non' if not result['fallback_mode'] else 'Oui'}")
            
            if "Qwen2.5" in result['deepinfra']['model']:
                print("ğŸ‰ Qwen2.5-72B correctement configurÃ© !")
            else:
                print("âš ï¸ Ancien modÃ¨le encore configurÃ©")
                
        else:
            print(f"âŒ Erreur: {response.status_code}")
            
    except Exception as e:
        print(f"âŒ Erreur: {e}")

if __name__ == "__main__":
    test_model_status()
    test_qwen_performance()
    test_qwen_vs_gemini_quality()
    
    print("\nğŸ‰ Tests Qwen2.5-72B terminÃ©s !")
    print("ğŸ’¡ Qwen2.5-72B devrait Ãªtre plus performant que Llama-3.3-70B")
    print("ğŸš€ Meilleur en franÃ§ais, plus rapide, et plus intelligent")
